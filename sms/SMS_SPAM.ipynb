{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "\n",
    "#EDA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#clean data\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#model accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.) Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the File from where the model will be trained.\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.txt', sep='\\t')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data['label'].value_counts()\n",
    "sns.barplot(x.index,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non SPAM Text(0) are More in the given dataset than the SPAM text(1)\n",
    "#### Dataset is biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING WORD-COUNT\n",
    "data['word_count'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=data[data['label']==1]['word_count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('SPAM text')\n",
    "\n",
    "train_words=data[data['label']==0]['word_count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-SPAM texts')\n",
    "fig.suptitle('Words per texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non SPAM Text(0) are longer than the SPAM text(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting label column into binary 0 or 1.\n",
    "\n",
    "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
    "data.head()\n",
    "\n",
    "\n",
    "def cleaning (text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])\n",
    "    text = re.split('W+',text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    text= [i for i in text if i not in stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "    \n",
    "\n",
    "\n",
    "data['text']= data['text'].apply(lambda x: cleaning(x))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.) Word to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(data['text'])\n",
    "vectorizer.transform(data['text']).toarray()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "X = data.text.values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test  = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_predict = classifier.predict(X_test)\n",
    "y_prob = classifier.predict_proba(X_test)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.) New String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = \"##type your SMS HERE\"\n",
    "#Pre-processing the new string\n",
    "testing = [cleaning (testing)]\n",
    "\n",
    "#converting words to numerical data using tf-idf\n",
    "X_vector = vectorizer.transform(testing)\n",
    "\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = classifier.predict(X_vector)      \n",
    "y_prob = classifier.predict_proba(X_vector)[:,1]\n",
    "\n",
    "print(y_predict)\n",
    "print(y_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
